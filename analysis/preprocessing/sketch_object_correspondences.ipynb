{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "## add helpers to python path\n",
    "if os.path.join('..','helpers') not in sys.path:\n",
    "    sys.path.append(os.path.join('..','helpers'))\n",
    "\n",
    "import pymongo as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageFilter\n",
    "import object_mask_utils as u\n",
    "import socket\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "from skimage import io, img_as_float\n",
    "import base64\n",
    "\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../../')\n",
    "analysis_dir = os.getcwd()\n",
    "data_dir = os.path.join(proj_dir,'data')\n",
    "experiment_dir = os.path.join(data_dir, 'experiment', 'refgame2.0')\n",
    "results_dir = os.path.join(data_dir,'diagnosticity')\n",
    "csv_dir = os.path.join(data_dir,'diagnosticity', 'refgame2.0')\n",
    "sketch_dir = os.path.abspath(os.path.join(data_dir,'sketches'))\n",
    "gallery_dir = os.path.abspath(os.path.join(results_dir,'gallery'))\n",
    "map_dir = os.path.abspath(os.path.join(results_dir,'maps'))\n",
    "mask_dir = os.path.join(results_dir, 'object_masks')\n",
    "\n",
    "## import dictionaries that map between shapenet ids and graphical conventions naming scheme\n",
    "importlib.reload(u)\n",
    "G2S = u.GC2SHAPENET\n",
    "S2G = u.SHAPENET2GC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paths to sketch and object maps\n",
    "sketch_map_dir = os.path.join(map_dir, 'single_sketch_maps')\n",
    "object_map_dir = os.path.join(map_dir, 'target_object_maps')\n",
    "\n",
    "## construct metadata object for sketch maps\n",
    "sketch_map_fnames = sorted(os.listdir(sketch_map_dir))\n",
    "S = pd.DataFrame(sketch_map_fnames)\n",
    "S.columns = ['filename']\n",
    "S = S.assign(path = S.apply(lambda x: os.path.join(sketch_map_dir, x['filename']), axis=1))\n",
    "S = S.assign(gameID = S.apply(lambda x: x['filename'].split('_')[0], axis=1))\n",
    "S = S.assign(target_id = (S.apply(lambda x: '{}_{}'\n",
    "                               .format(x['filename']\n",
    "                                       .split('_')[1], x['filename']\n",
    "                                       .split('_')[2]), axis=1)))\n",
    "S = S.assign(repetition = S.apply(lambda x: int(str(x['filename'].split('_')[3].split('.')[0]).zfill(2)), axis=1))\n",
    "S = S.assign(shapenet_id = S.apply(lambda x: G2S[x['target_id']], axis=1))\n",
    "S = S.assign(gameID_target = S.apply(lambda x: '{}_{}'.format(x['gameID'], x['target_id']), axis=1))\n",
    "S.sort_values(by=['gameID','target_id','repetition'], inplace=True)\n",
    "print('There are {} annotated sketches.'.format(S.shape[0]))\n",
    "\n",
    "## construct metadata object for target maps\n",
    "object_map_fnames = sorted(os.listdir(object_map_dir))\n",
    "O = pd.DataFrame(object_map_fnames)\n",
    "O.columns = ['filename']\n",
    "O = O.assign(target_id = O.apply(lambda x: x['filename'].split('.')[0], axis=1))\n",
    "O = O.assign(shapenet_id = O.apply(lambda x: G2S[x['target_id']], axis=1))\n",
    "\n",
    "## load in group metadata\n",
    "path_to_meta = os.path.join(experiment_dir,'graphical_conventions_group_data_run5_submitButton.csv')\n",
    "M = pd.read_csv(path_to_meta)\n",
    "M = M.assign(gameID_target = M.apply(lambda x: '{}_{}'.format(x['gameID'],  x['target']), axis=1))\n",
    "\n",
    "## convenience dictionary to map from gameID/target combinations to condition labels\n",
    "GT2C = dict(zip(M.gameID_target.values, M.condition.values))\n",
    "## add condition column to S so we can easily subset on condition\n",
    "S = S.assign(condition = S.apply(lambda x: GT2C[x['gameID_target']], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute diagnosticity of sketches over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_sketch_map_inds = []\n",
    "missing_sketch_map_inds = []\n",
    "counter = 1\n",
    "sketch_map_intensity_tally = []\n",
    "for name, group in M.groupby(['gameID','target','repetition']):\n",
    "\n",
    "    assert len(group.index)==1\n",
    "    print('{} | Now computing diagnosticity for: {} {} {}'.format(counter,name[0], name[1], name[2]))\n",
    "    clear_output(wait=True)\n",
    "    counter+=1\n",
    "    \n",
    "    ## grab the corresponding row of the dataframe\n",
    "    s = S[(S['gameID']==name[0]) & (S['target_id']==name[1]) & (S['repetition']==name[2])]\n",
    "    o = O[O['target_id']==name[1]]\n",
    "    \n",
    "    ## only proceed if there is an annotated sketch here\n",
    "    if s.shape[0]!=1:\n",
    "        missing_sketch_map_inds.append(group.index[0])\n",
    "    else:\n",
    "        ## get path to load in raw mappings\n",
    "        sketch_map_path = os.path.join(sketch_map_dir, s['filename'].values[0])\n",
    "        object_map_path = os.path.join(object_map_dir, o['filename'].values[0])            \n",
    "        mask_path = os.path.join(mask_dir, '{}-mask.png'.format(s['target_id'].values[0]))\n",
    "        \n",
    "        ## load in images \n",
    "        sketch_map = Image.open(sketch_map_path).convert('L')\n",
    "        object_map = Image.open(object_map_path).convert('L')\n",
    "        object_mask = Image.open(mask_path).resize((300,300),Image.LANCZOS)\n",
    "        sketch_map_intensity_tally.append(np.mean(sketch_map))\n",
    "        \n",
    "        if np.mean(sketch_map) < 1: ## check if invalid/empty sketch map; only proceed if valid, non-empty map\n",
    "            invalid_sketch_map_inds.append(group.index[0])\n",
    "        else:\n",
    "            ## multiply sketch and object maps together\n",
    "            combo_array = np.array(sketch_map).astype(np.float32) * np.array(object_map).astype(np.float32) / 255\n",
    "            sketch_object_combo = Image.fromarray(combo_array.astype(np.uint16)).convert('L')      \n",
    "#             sketch_object_combo = ImageChops.multiply(sketch_map,object_map)\n",
    "            \n",
    "            ## multiply sketchxobject and object MASK together\n",
    "            masked_combo_array = combo_array * object_mask / 255\n",
    "            masked_combo = Image.fromarray(masked_combo_array.astype(np.uint16)).convert('L')               \n",
    "                    \n",
    "            ## save out convolved object x sketch maps\n",
    "            out_dir = os.path.join(map_dir,'object_sketch_maps')\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir) \n",
    "            out_path = os.path.join(out_dir,'{}_{}_{}.png'.format(name[0], name[1], str(name[2]).zfill(2)))\n",
    "            masked_combo.save(out_path)    \n",
    "\n",
    "            ## total intensity of sketch_map, normalize diagnosticity by this\n",
    "            sketch_map_total_intensity = np.sum(sketch_map)/255\n",
    "            M.loc[group.index[0],'intensity'] = sketch_map_total_intensity             \n",
    "\n",
    "            ## compute raw sketch diagnosticity\n",
    "            sketch_diagnosticity = np.sum(masked_combo)/255\n",
    "            M.loc[group.index[0],'diagnosticity_raw'] = sketch_diagnosticity             \n",
    "            \n",
    "            ## compute normalized sketch diagnosticity\n",
    "            sketch_diagnosticity = np.sum(masked_combo)/255/sketch_map_total_intensity\n",
    "            M.loc[group.index[0],'diagnosticity'] = sketch_diagnosticity \n",
    "            \n",
    "            ## compute entropy of sketch-object composite\n",
    "            sketch_entropy = entropy(np.array(masked_combo).flatten())\n",
    "            M.loc[group.index[0],'entropy'] = sketch_entropy \n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save out invalid sketch map inds so that we can collect higher quality maps on them\n",
    "M.iloc[invalid_sketch_map_inds].to_csv(os.path.join(csv_dir,'invalid_stroke_maps.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} invalid/blank sketch maps'.format(len(invalid_sketch_map_inds)))\n",
    "print('{} missing sketch maps'.format(len(missing_sketch_map_inds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(sketch_map_intensity_tally,bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save out diagnosticity tidy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.to_csv(os.path.join(csv_dir,'graphical_conventions_group_data_run5_submitButton_diagnosticity.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick diagnosticity analyses & plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(26,8))\n",
    "plt.subplot(131)\n",
    "sns.lineplot(data=M, x='repetition', y='entropy', hue='condition')\n",
    "plt.subplot(132)\n",
    "sns.lineplot(data=M, x='repetition', y='diagnosticity', hue='condition')\n",
    "plt.subplot(133)\n",
    "sns.lineplot(data=M, x='repetition', y='intensity', hue='condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IOU (intersection over union) for each pair of sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all combinations\n",
    "num_reps = 8 \n",
    "mesh = np.array(np.meshgrid(np.arange(num_reps), np.arange(num_reps)))\n",
    "combos = mesh.T.reshape(-1, 2)\n",
    "\n",
    "## subset to just repeated condition\n",
    "SR = S[S['condition']=='repeated']\n",
    "\n",
    "## init XSECT & YSECT group heatmaps\n",
    "XSECT = [] # diagnosticity of what is shared between sketches (intersection)\n",
    "YSECT = [] # diagnosticity of what is lost between early and late sketches (setdiff)\n",
    "counter = 1\n",
    "\n",
    "## init group dataframe\n",
    "I2 = pd.DataFrame()\n",
    "\n",
    "for name, group in SR.groupby(['gameID', 'target_id']):\n",
    "    if SR[SR['gameID']==name[0]].shape[0]==32: ## only analyze those that we have complete annotations for\n",
    "        print('{} | Analyzing {} {}'.format(counter, name[0], name[1]))\n",
    "        counter +=1\n",
    "        clear_output(wait=True)\n",
    "        I = pd.DataFrame()\n",
    "        I['arr1']=None\n",
    "        I['arr2']=None\n",
    "        I['xsect']=None\n",
    "        I['xsect_obj']=None\n",
    "        I['union']=None    \n",
    "        I['im1not2']=None\n",
    "        I['im1not2_obj'] = None\n",
    "        I['im2not1']=None\n",
    "\n",
    "        sid = group['target_id'].unique()[0]\n",
    "\n",
    "        ## extract object map (ground truth diagnosticity for this object)\n",
    "        obj_path = os.path.join(object_map_dir,'{}.png'.format(sid))\n",
    "        obj_map = np.array(Image.open(obj_path)).astype(np.float32).flatten()    \n",
    "\n",
    "        for combo_ind, this_combo in enumerate(combos):\n",
    "\n",
    "            ## unique identifier for each gameID/target combo\n",
    "            I.loc[combo_ind, 'obj_id'] = group['gameID_target'].unique()[0]    \n",
    "            I.loc[combo_ind, 'im1_index'] = np.int(this_combo[0])\n",
    "            I.loc[combo_ind, 'im2_index'] = np.int(this_combo[1])\n",
    "\n",
    "            im1_path = group.iloc[this_combo[0]]['path']\n",
    "            im2_path = group.iloc[this_combo[1]]['path']\n",
    "\n",
    "            im1 = Image.open(im1_path)\n",
    "            im2 = Image.open(im2_path)\n",
    "\n",
    "            arr1 = np.array(im1).flatten()\n",
    "            arr2 = np.array(im2).flatten()\n",
    "            I.at[combo_ind,'arr1'] = arr1\n",
    "            I.at[combo_ind,'arr2'] = arr2\n",
    "            I.loc[combo_ind,'arr1_size'] = np.sum(arr1)\n",
    "            I.loc[combo_ind,'arr2_size'] = np.sum(arr2)        \n",
    "            ## get individual diagnosticity\n",
    "            arr1_obj = (obj_map * arr1) / 255\n",
    "            arr2_obj = (obj_map * arr2) / 255\n",
    "            arr1_total = np.sum(arr1)\n",
    "            arr2_total = np.sum(arr2)\n",
    "            I.loc[combo_ind,'arr1_obj_diagnosticity'] = np.sum(arr1_obj)/arr1_total\n",
    "            I.loc[combo_ind,'arr2_obj_diagnosticity'] = np.sum(arr2_obj)/arr2_total\n",
    "            I.loc[combo_ind,'diagnosticity_change'] = np.sum(arr2_obj)/arr2_total - np.sum(arr1_obj)/arr1_total\n",
    "\n",
    "            ## get intersection (sketch content in im2 that is also in im1)\n",
    "            xsect = np.logical_and(arr1, arr2)\n",
    "            I.at[combo_ind,'xsect'] = xsect\n",
    "            I.loc[combo_ind,'xsect_size'] = np.sum(xsect)  \n",
    "\n",
    "            ## get union (sketch content in either im1 or im2)\n",
    "            union = np.logical_or(arr1, arr2)\n",
    "            I.at[combo_ind,'union'] = xsect\n",
    "            I.loc[combo_ind,'union_size'] = np.sum(union)         \n",
    "                        \n",
    "            ## get intersection x object diagnosticity map\n",
    "            xsect_obj = (obj_map * xsect) / 255\n",
    "            I.at[combo_ind,'xsect_obj'] = xsect_obj        \n",
    "            I.loc[combo_ind,'xsect_obj_sum'] = np.sum(xsect_obj)            \n",
    "\n",
    "            ## get inverse of each image\n",
    "            inv1 = np.invert(arr1)\n",
    "            inv2 = np.invert(arr2)\n",
    "\n",
    "            ## get A-not-B set diff (sketch content in im1 that no longer appears in im2)\n",
    "            im1not2 = np.logical_and(arr1, inv2)\n",
    "            I.at[combo_ind,'im1not2'] = im1not2\n",
    "            I.loc[combo_ind,'im1not2_size']  = np.sum(im1not2)\n",
    "\n",
    "            ## get A-not-B x object diagnosticity map\n",
    "            im1not2_obj = (obj_map * im1not2) / 255\n",
    "            I.at[combo_ind,'im1not2_obj'] = im1not2_obj        \n",
    "            I.loc[combo_ind,'im1not2_obj_sum'] = np.sum(im1not2_obj)     \n",
    "\n",
    "            ## get B-not-A set diff (new sketch content that appears in im2 that was not in im1)\n",
    "            im2not1 = np.logical_and(arr2, inv1)\n",
    "            I.at[combo_ind,'im2not1'] = im2not1\n",
    "            I.loc[combo_ind,'im2not1_size']  = np.sum(im2not1)\n",
    "\n",
    "            ## get IOU (intersection over union)\n",
    "            I.loc[combo_ind,'IOU'] = I.loc[combo_ind, 'xsect_size'] / I.loc[combo_ind, 'union_size']\n",
    "            I.loc[combo_ind,'AOU'] = I.loc[combo_ind, 'im1not2_size'] / I.loc[combo_ind, 'union_size']\n",
    "            I.loc[combo_ind,'BOU'] = I.loc[combo_ind, 'im2not1_size'] / I.loc[combo_ind, 'union_size']\n",
    "\n",
    "        I = (I.astype({'im1_index': 'int32', 'im2_index': 'int32', 'xsect_size':'int32',\n",
    "                        'im1not2_size': 'int32', 'im2not1_size': 'int32',\n",
    "                        'arr1_size': 'int32', 'arr2_size': 'int32', 'union_size': 'int32'}))    \n",
    "\n",
    "        ## create intersection heatmap\n",
    "        X = np.zeros((num_reps,num_reps)) ## init d matrix \n",
    "        for i,d in I.iterrows():\n",
    "            if (d['arr1_size']==0) or (d['arr2_size']==0) or (d['xsect_size']==0):\n",
    "                X[d['im1_index'], d['im2_index']] = np.nan\n",
    "            else:\n",
    "                X[d['im1_index'], d['im2_index']] = d['xsect_obj_sum'] / d['xsect_size']        \n",
    "        XSECT.append(X)\n",
    "\n",
    "        # create A-not-B set-diff heatmap\n",
    "        Y = np.zeros((num_reps,num_reps)) ## init d matrix \n",
    "        for i,d in I.iterrows():\n",
    "            if (d['arr1_size']==0) or (d['arr2_size']==0) or (d['im1not2_size']==0):\n",
    "                Y[d['im1_index'], d['im2_index']] = np.nan\n",
    "            else:\n",
    "                Y[d['im1_index'], d['im2_index']] = d['im1not2_obj_sum'] / d['im1not2_size']        \n",
    "        YSECT.append(Y)  \n",
    "\n",
    "        ## get aggregate dataframe for whole group\n",
    "        if len(I2) == 0:\n",
    "            I2 = I\n",
    "        else:\n",
    "            I2 = pd.concat([I2, I], axis=0)\n",
    "\n",
    "\n",
    "## convert to numpy array\n",
    "XSECT = np.array(XSECT) \n",
    "YSECT = np.array(YSECT) \n",
    "I2 = I2.reset_index(drop=True)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save these results to be analyzed in R (random intercepts for targets, gameIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'graphical_conventions_group_data_run5_submitButton_sketchOverlap.csv'\n",
    "I2[['obj_id', 'diagnosticity_change', 'IOU', 'AOU', 'BOU']].to_csv(os.path.join(csv_dir,'diagnosticity.csv'),index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just pick the columns that aren't huge (i.e., that aren't image arrays)\n",
    "I3 = (I2[['obj_id', 'im1_index', 'im2_index', 'arr1_size', 'arr2_size', 'xsect_size',\n",
    "          'xsect_obj_sum', 'union_size','im1not2_size','im1not2_obj_sum', 'im2not1_size']])\n",
    "\n",
    "## save out\n",
    "out_path = os.path.join(csv_dir, 'paired_sketch_diagnosticity_repeated.csv')\n",
    "I3.to_csv(out_path, index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize mean diagnosticity for four cases:\n",
    "\n",
    "- (1) whole sketches across repetitions \n",
    "- (2) partial sketches that are shared between pairs of sketches (intersection btw A & B)\n",
    "- (3) partial sketches that appear in early sketches but NOT in later ones (A not B)\n",
    "- (4) partial sketches that appear in late sketches but NOT in early ones (B not A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1: whole sketches across repetitions**\n",
    "    - the diagonal of this matrix\n",
    "    \n",
    "**Case 2: partial sketches that are shared between pairs of sketches (intersection btw A & B)**\n",
    "    - the off-diagonal cells. \n",
    "    - from left to right, row 1 shows how mean diagnosticity of intersection between first sketch and subsequent sketches evolves across repetitions\n",
    "    - the parts of later sketches that are shared with early sketches are higher in diagnosticity than the parts of earlier sketches shared with even earlier sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.nanmean(XSECT,axis=0), cmap='viridis') #, vmin=0.3, vmax=0.45\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3: partial sketches that appear in early sketches but NOT in later ones (A not B)**\n",
    "    - the upper triangle of this matrix\n",
    "    - from left to right, row 1 shows how mean diagnosticity of parts of sketches that \"drop out\" after 1st sketch evolves across repetitions    \n",
    "**Case 4: partial sketches that appear in late sketches but NOT in early ones (B not A)**\n",
    "    - the lower triangle of this matrix\n",
    "    - from top to bottom, column 1 shows how mean diagnosticity of parts of sketches that \"pop up\" after 1st sketch (but do NOT appear in 1st sketch) evolves across repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.nanmean(YSECT,axis=0), cmap='viridis') #, vmin=0.3, vmax=0.45\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
