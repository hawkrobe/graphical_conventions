{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/megsano/.local/lib/python2.7/site-packages/cryptography/hazmat/primitives/constant_time.py:26: CryptographyDeprecationWarning: Support for your Python version is deprecated. The next version of cryptography will remove support. Please upgrade to a 2.7.x release that supports hmac.compare_digest as soon as possible.\n",
      "  utils.DeprecatedIn23,\n",
      "Requirement already satisfied: argparse in /usr/lib/python2.7 (1.2.1)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../python/')\n",
    "from embeddings import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "!pip install argparse\n",
    "import argparse\n",
    "\n",
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "#drawings_dir = os.path.join(sketch_dir,'run3_run4')\n",
    "drawings_dir = os.path.join(os.path.join(sketch_dir, 'stroke_analysis'), 'png')\n",
    "#h.save_sketches(D_run3_correct, complete_games, sketch_dir, 'run3_run4', iterationNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve sketch paths\n",
    "def list_files(path, ext='png'):\n",
    "    result = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "    return result\n",
    "\n",
    "def check_invalid_sketch(filenames,invalids_path='drawings_to_exclude.txt'):    \n",
    "    if not os.path.exists(invalids_path):\n",
    "        print('No file containing invalid paths at {}'.format(invalids_path))\n",
    "        invalids = []        \n",
    "    else:\n",
    "        x = pd.read_csv(invalids_path, header=None)\n",
    "        x.columns = ['filenames']\n",
    "        invalids = list(x.filenames.values)\n",
    "    valids = []   \n",
    "    basenames = [f.split('/')[-1] for f in filenames]\n",
    "    for i,f in enumerate(basenames):\n",
    "        if f not in invalids:\n",
    "            valids.append(filenames[i])\n",
    "    return valids\n",
    "\n",
    "def make_dataframe(Labels):    \n",
    "    Y = pd.DataFrame(Labels)\n",
    "    Y = Y.transpose()\n",
    "    Y.columns = ['label']\n",
    "    return Y\n",
    "\n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "def preprocess_features(Features, Y):\n",
    "    _Y = Y.sort_values(['label'])\n",
    "    inds = np.array(_Y.index)\n",
    "    _Features = normalize(Features[inds])\n",
    "    _Y = _Y.reset_index(drop=True) # reset pandas dataframe index\n",
    "    return _Features, _Y\n",
    "\n",
    "def save_features(Features, Y, layer_num, data_type,feat_path='./sketch_features'):\n",
    "    if not os.path.exists('./sketch_features'):\n",
    "        os.makedirs('./sketch_features')\n",
    "    layers = ['P1','P2','P3','P4','P5','FC6','FC7']\n",
    "    np.save(os.path.join(feat_path,'FEATURES_{}_{}_stroke_analysis.npy'.format(layers[int(layer_num)], data_type)), Features)\n",
    "    Y.to_csv(os.path.join(feat_path,'METADATA_{}_stroke_analysis.csv'.format(data_type)))\n",
    "    return layers[int(layer_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_paths before filtering: 1841\n",
      "No file containing invalid paths at drawings_to_exclude.txt\n",
      "Length of image_paths after filtering: 1841\n",
      "CUDA DEVICE NUM: 0\n",
      "Batch 1\n",
      "batch size: 64\n",
      "Batch 2\n",
      "batch size: 64\n",
      "Batch 3\n",
      "batch size: 64\n",
      "Batch 4\n",
      "batch size: 64\n",
      "Batch 5\n",
      "batch size: 64\n",
      "Batch 6\n",
      "batch size: 64\n",
      "Batch 7\n",
      "batch size: 64\n",
      "Batch 8\n",
      "batch size: 64\n",
      "Batch 9\n",
      "batch size: 64\n",
      "Batch 10\n",
      "batch size: 64\n",
      "Batch 11\n",
      "batch size: 64\n",
      "Batch 12\n",
      "batch size: 64\n",
      "Batch 13\n",
      "batch size: 64\n",
      "Batch 14\n",
      "batch size: 64\n",
      "Batch 15\n",
      "batch size: 64\n",
      "Batch 16\n",
      "batch size: 64\n",
      "Batch 17\n",
      "batch size: 64\n",
      "Batch 18\n",
      "batch size: 64\n",
      "Batch 19\n",
      "batch size: 64\n",
      "Batch 20\n",
      "batch size: 64\n",
      "Batch 21\n",
      "batch size: 64\n",
      "Batch 22\n",
      "batch size: 64\n",
      "Batch 23\n",
      "batch size: 64\n",
      "Batch 24\n",
      "batch size: 64\n",
      "Batch 25\n",
      "batch size: 64\n",
      "Batch 26\n",
      "batch size: 64\n",
      "Batch 27\n",
      "batch size: 64\n",
      "Batch 28\n",
      "batch size: 64\n",
      "Batch 29\n",
      "batch size: 64\n",
      "stopped!\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, help='full path to images', default=drawings_dir)#'combined_final_png_drawings')\n",
    "parser.add_argument('--layer_ind', help='fc6 = 5, fc7 = 6', default=5)\n",
    "parser.add_argument('--data_type', help='\"images\" or \"sketch\"', default='sketch')\n",
    "parser.add_argument('--spatial_avg', type=bool, help='collapse over spatial dimensions, preserving channel activation only if true', default=False)     \n",
    "parser.add_argument('--test', type=bool, help='testing only, do not save features', default=False)  \n",
    "parser.add_argument('--ext', type=str, help='image extension type (e.g., \"png\")', default=\"png\")   \n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args = parser.parse_args(argv[1:])\n",
    "\n",
    "## get list of all sketch paths\n",
    "image_paths = sorted(list_files(args.data,args.ext))\n",
    "print('Length of image_paths before filtering: {}'.format(len(image_paths)))\n",
    "\n",
    "## filter out invalid sketches\n",
    "image_paths = check_invalid_sketch(image_paths)\n",
    "print('Length of image_paths after filtering: {}'.format(len(image_paths)))    \n",
    "\n",
    "## extract features\n",
    "layers = ['P1','P2','P3','P4','P5','FC6','FC7']\n",
    "extractor = FeatureExtractor(image_paths,layer=args.layer_ind,data_type=args.data_type,spatial_avg=args.spatial_avg)\n",
    "Features, Labels = extractor.extract_feature_matrix()   \n",
    "# # organize metadata into dataframe\n",
    "# Y = make_dataframe(Labels)\n",
    "Y = pd.DataFrame(Labels)\n",
    "Y = Y.transpose()\n",
    "Y.columns = ['0', '1', '2', '3', '4', '5', '6']\n",
    "m = pd.DataFrame()\n",
    "label_list = []\n",
    "for i,d in Y.iterrows():\n",
    "    label = d['0'] + '_' + d['1'] + '_' + d['2'] + '_' + d['3'] + '_' + d['4'] + '_' + d['5'] + '_' + d['6']\n",
    "    label_list.append(label)\n",
    "m['label'] = label_list\n",
    "\n",
    "# if args.test==False:\n",
    "#     layer = save_features(_Features, _Y, args.layer_ind, args.data_type) # g,trialNum,target, repetition, iterationNum\n",
    "_Features, _Y = preprocess_features(Features, m)\n",
    "\n",
    "if args.test==False:\n",
    "    layer = save_features(_Features, _Y, args.layer_ind, args.data_type) # g,trialNum,target, repetition, iterationNum\n",
    "#Y.to_csv('feature_matrix_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### manually save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_path = './sketch_features'\n",
    "layer_num = args.layer_ind\n",
    "data_type = 'sketch'\n",
    "layers = ['P1','P2','P3','P4','P5','FC6','FC7']\n",
    "out_path = os.path.join(feat_path,'FEATURES_{}_{}_stroke_analysis.npy'.format(layers[int(layer_num)], data_type))\n",
    "np.save(out_path, _Features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
