{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis','python') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis','python'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_run3_unnormalized = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run3', 'unnormalized')))\n",
    "D_run4_unnormalized = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run4', 'unnormalized')))\n",
    "D_run3_normalized = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run3', 'normalized')))\n",
    "D_run4_normalized = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run4', 'normalized')))\n",
    "D_run3_raw = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run3', 'raw')))\n",
    "D_run4_raw = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run4', 'raw')))\n",
    "D_run3_filtered = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run3', 'filtered')))\n",
    "D_run4_filtered = pd.read_csv(os.path.join(results_dir,'graphical_conventions_{}_{}.csv'.format('run4', 'filtered')))\n",
    "D_unnormalized = pd.concat([D_run3_unnormalized, D_run4_unnormalized], axis = 0)\n",
    "D_normalized = pd.concat([D_run3_normalized, D_run4_normalized], axis = 0)\n",
    "D_raw =  pd.concat([D_run3_raw, D_run4_raw], axis = 0)\n",
    "D_filtered =  pd.concat([D_run3_filtered, D_run4_filtered], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load sketch features + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(h)\n",
    "path_to_feats = './sketch_features/FEATURES_FC6_sketch.npy' # '/home/megsano/combined_sketch_features/FEATURES_FC6_sketch.npy'\n",
    "path_to_meta = './sketch_features/METADATA_sketch.csv' # '/home/megsano/combined_sketch_features/METADATA_sketch.csv'\n",
    "\n",
    "F = np.load(path_to_feats)\n",
    "M = pd.read_csv(path_to_meta)\n",
    "\n",
    "assert F.shape[0]==M.shape[0]\n",
    "M = h.clean_up_metadata(M) \n",
    "M = h.split_up_metadata(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. within-interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what's the mean accuracy?\n",
    "total = 0\n",
    "for outcome in D_filtered['outcome']:\n",
    "    total = total + outcome \n",
    "meanAccuracy = total / len(D_filtered['outcome'])\n",
    "print (\"accuracy: {}\".format(meanAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot mean accuracy across repetitions \n",
    "reload(h)\n",
    "h.plot_accuracy_reps(D_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cost metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### across repetitions \n",
    "reload(h)\n",
    "var0='numStrokes'\n",
    "var1='drawDuration'\n",
    "var2='numCurvesPerSketch'\n",
    "var3= 'meanPixelIntensity'\n",
    "h.ts_grid_repeated_control(D_normalized, \n",
    "                                                var0, var1, var2, var3,\n",
    "                                                numReps=8,\n",
    "                                                 save_plot=False,\n",
    "                                                 plot_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### pre and post comparison \n",
    "reload(h)\n",
    "D1 = h.compare_conditions_prepost(D_normalized,\n",
    "                                var='numStrokes',\n",
    "                                lower_limit = 4,\n",
    "                                upper_limit=10,\n",
    "                                save_plot=False,\n",
    "                                plot_dir=plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### efficiency (based on cost and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(h)\n",
    "h.plot_bis_scores(D_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sketch similarity convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### create matrix for similarity between adjacent repetitions averaged within target and within game\n",
    "# reload(h)\n",
    "# arr_of_cormats = h.make_adjacency_matrix(M, F, 'gameID')\n",
    "\n",
    "result = np.zeros((8, 8))\n",
    "count = 0\n",
    "F_ = np.vstack((F, [float('NaN')] * 4096))\n",
    "arr_of_corrmats = []\n",
    "for game in M[gameID].unique(): #['3480-03933bf3-5e7e-4ecd-b151-7ae57e6ae826']:\n",
    "    for target in (M[M[gameID] == game])['target'].unique():  #['dining_04']:\n",
    "        count = count + 1\n",
    "        M_isolated = M[(M[gameID] == game) & (M['target'] == target)]\n",
    "        for rep in range(8):\n",
    "            if rep not in list(M_isolated['repetition']):\n",
    "                df_to_add = pd.DataFrame([[game, float('NaN'), rep, target, len(F)]], columns=[gameID, 'trialNum', 'repetition', 'target', 'feature_ind'])\n",
    "                M_isolated = M_isolated.append(df_to_add)\n",
    "        M_isolated_sorted = M_isolated.sort_values(by=['repetition'])\n",
    "        inds_to_compare = M_isolated_sorted['feature_ind']\n",
    "        features_to_compare = F_[inds_to_compare, :]\n",
    "\n",
    "        # add features to a new dataframe \n",
    "        # and compute corr with pandas to handle NaN well  \n",
    "        features_df = pd.DataFrame()\n",
    "        column_index = 0\n",
    "        for row in features_to_compare:\n",
    "            features_df[str(column_index)] = pd.Series(list(row))\n",
    "            column_index = column_index + 1\n",
    "\n",
    "        pd_CORRMAT = features_df.corr()\n",
    "        np_CORRMAT = pd_CORRMAT.values\n",
    "        arr_of_corrmats.append(np_CORRMAT)\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        to_add = [mat[i][j] for mat in arr_of_corrmats]\n",
    "        result[i][j] = np.nanmean(np.array(to_add))\n",
    "\n",
    "average_corr_mat = np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M[M['gameID'] == '9817-6f2a66db-fc24-482d-aa20-1980ad6f524e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h.plot_within_interaction_similarity(arr_of_corrmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### permute across repetitions and create matrix for similarity between adjacent repetitions averaged within target and within game\n",
    "M_pseudo = h.scramble_df_within_target_rep(M)\n",
    "arr_of_corrmats_permuted = h.make_adjacency_matrix(M_pseudo, 'pseudo_gameID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### just plotting one back similarities \n",
    "h.plot_within_interaction_similarity(arr_of_corrmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. between-interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sketch similarity divergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### calculates similarity of sketches between interactions within a repetition \n",
    "h.plot_between_interaction_similarity(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplementary analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### printing sketches across repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print repeated sketches (with actual image)\n",
    "reload(h)\n",
    "h.print_repeated_actual(D_run3,\n",
    "                                   _complete_games,\n",
    "                                   4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print repeated sketches (with control sketches)\n",
    "reload(h)\n",
    "h.print_repeated_control(D_run3,\n",
    "                                   _complete_games,\n",
    "                                  4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting 8 RDMs, one for each repetition \n",
    "fig, axs = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(40,20))\n",
    "for rep in range(8): \n",
    "    x_ind = rep % 4\n",
    "    y_ind = 0 if rep < 4 else 1 \n",
    "    M_rep = M[M['repetition'] == str(int(rep))]\n",
    "    M_rep_sorted = M_rep.sort_values(by=['target'])\n",
    "    sorted_feature_ind = list(M_rep_sorted['feature_ind'])\n",
    "    get_and_plot_RDM(M_rep, F, sorted_feature_ind, axs, x_ind, y_ind, rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### path-dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corr_coefs(M,F):\n",
    "    auto_corr_df = pd.DataFrame()\n",
    "    for g in list(M['gameID'].unique()):\n",
    "        for t in list(M[M['gameID'] == g]['target'].unique()):\n",
    "            mini_df = M[(M['gameID'] == g) & (M['target'] == t)] \n",
    "            #mini_df = M[(M['gameID'] == '0050-769c4742-aefc-4bea-aeb2-7dc48b51fe82') & (M['target'] == 'dining_05')] \n",
    "            for base_rep in mini_df['repetition'].unique():\n",
    "                base_ind = list(mini_df[mini_df['repetition'] == base_rep]['feature_ind'])[0]\n",
    "                for other_rep in mini_df['repetition'].unique():\n",
    "                    if base_rep < other_rep:# check if other_rep is bigger - only compare forward \n",
    "                        rep_dist = other_rep - base_rep\n",
    "                        other_ind = list(mini_df[mini_df['repetition'] == other_rep]['feature_ind'])[0]\n",
    "                        features_to_compare = F[[base_ind, other_ind], :]\n",
    "                        corr_coef = np.corrcoef(features_to_compare)[1][0]\n",
    "                        df_to_add = pd.DataFrame([[g, t, base_rep,  rep_dist, corr_coef]], columns=['gameID', 'target', 'base_rep', 'rep_dist', 'corr_coef'])\n",
    "                        auto_corr_df = auto_corr_df.append(df_to_add)\n",
    "    return auto_corr_df\n",
    "\n",
    "auto_corr_df = get_corr_coefs(M, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how does the similarity between sketches of adjacent repetitions change with increasing base repetition number? \n",
    "sns.regplot(\n",
    "    data=auto_corr_df,\n",
    "    x_estimator=np.mean,\n",
    "    x='base_rep',\n",
    "    y='corr_coef',\n",
    "    logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how does the similarity between sketches of adjacent repetitions change with increasing repetition distance? \n",
    "sns.regplot(\n",
    "    data=auto_corr_df,\n",
    "    x_estimator=np.mean,\n",
    "    x='rep_dist',\n",
    "    y='corr_coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df_ = pd.DataFrame()\n",
    "for rep in auto_corr_df['base_rep'].unique():\n",
    "    df_ = auto_corr_df[auto_corr_df['base_rep'] == rep]\n",
    "    for dist in df_['rep_dist'].unique():\n",
    "        _df = df_[df_['rep_dist'] == dist]\n",
    "        mean = np.mean(np.array(_df['corr_coef']))\n",
    "        df_to_add = pd.DataFrame([[rep, dist, mean]], columns=['base_rep', 'rep_dist', 'mean_corr_coef'])\n",
    "        new_df_ = new_df_.append(df_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=new_df_, col=\"rep_dist\", height=3, aspect=1, col_wrap=4, margin_titles=True)\n",
    "g = g.map(plt.scatter, \"base_rep\", \"mean_corr_coef\")\n",
    "# g = sns.catplot(x=\"base_rep\", y=\"mean_corr_coef\",\n",
    "#                 col=\"rep_dist\", data=new_df_,col_wrap=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### classifier approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sklearn\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics  import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataframe with just the first repetition \n",
    "M_0 = M[M['repetition'] == 0]\n",
    "y = list(M_0['target'])\n",
    "feature_inds = list(M_0['feature_ind'])\n",
    "features = F[feature_inds, :]\n",
    "x = pd.DataFrame(features)\n",
    "\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "# make dataframe with just the last repetition \n",
    "M_7 = M[M['repetition'] == 7]\n",
    "y = list(M_7['target'])\n",
    "feature_inds = list(M_7['feature_ind'])\n",
    "features = F[feature_inds, :]\n",
    "x = pd.DataFrame(features)\n",
    "\n",
    "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes training and testing sets for sketches from specific repetition \n",
    "def make_sets(M, rep):\n",
    "    # make dataframe with just the last repetition \n",
    "    M_= M[M['repetition'] == rep]\n",
    "    y = list(M_['target'])\n",
    "    feature_inds = list(M_['feature_ind'])\n",
    "    features = F[feature_inds, :]\n",
    "    x = pd.DataFrame(features)\n",
    "    return train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make either training set or testing set (all sketches used for either one of them)\n",
    "def get_x_and_y(M, rep):\n",
    "    M_= M[M['repetition'] == rep]\n",
    "    y = list(M_['target'])\n",
    "    feature_inds = list(M_['feature_ind'])\n",
    "    features = F[feature_inds, :]\n",
    "    x = pd.DataFrame(features)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_sketches(train_rep, test_rep):\n",
    "    summary_df = pd.DataFrame()\n",
    "    for i in range(1):\n",
    "    \n",
    "        # first train on the train_rep sketches\n",
    "        X_train_, y_train_= get_x_and_y(M, train_rep)\n",
    "        logreg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "        #clf = CalibratedClassifierCV(log) \n",
    "        logreg.fit(X_train_, y_train_)\n",
    "#         svc = LinearSVC(random_state=0)\n",
    "#         clf = CalibratedClassifierCV(svc) \n",
    "#         clf.fit(X_train_, y_train_)\n",
    "\n",
    "        # for each repetition, predict from features, record accuracy and class probability    \n",
    "        for rep in test_rep:\n",
    "            X_test, y_test = get_x_and_y(M, rep)\n",
    "            y_pred = logreg.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prob = logreg.predict_proba(X_test)\n",
    "            prob_df_svc = pd.DataFrame(prob)\n",
    "            prob_df_svc.columns=list(logreg.classes_)\n",
    "            prob_df_svc['target'] = y_test\n",
    "            prob_list = []\n",
    "            prob_df_svc = prob_df_svc.reset_index(drop=True)\n",
    "            for j, d in prob_df_svc.iterrows():\n",
    "                target = d['target']\n",
    "                class_prob = d[target]\n",
    "                prob_list.append(class_prob)\n",
    "            mean_prob = np.mean(prob_list)\n",
    "\n",
    "            df_to_add = pd.DataFrame([[i, rep, acc, mean_prob]], columns=['iteration','test_rep', 'accuracy', 'class_prob'])\n",
    "            summary_df = summary_df.append(df_to_add)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_df0 = classify_sketches(0, [1,7])\n",
    "summary_df7 = classify_sketches(7, [6,0])\n",
    "summary_df0['train_rep'] = [0] * summary_df0.shape[0]\n",
    "summary_df7['train_rep'] = [7] * summary_df7.shape[0]\n",
    "summary_df_list = [summary_df0, summary_df7]\n",
    "combined_df = pd.concat(summary_df_list, axis=0)\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "distance_list = []\n",
    "for i,d in combined_df.iterrows():\n",
    "    distance_list.append(abs(d['test_rep']-d['train_rep']))\n",
    "combined_df['distance'] = distance_list\n",
    "combined_df = combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.pointplot(\n",
    "    data=combined_df,\n",
    "    x='distance',\n",
    "    y='accuracy',\n",
    "    hue='train_rep',\n",
    "    palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x_and_y_permute(M, rep):\n",
    "    M_= M[M['pseudo_repetition'] == rep]\n",
    "    y = list(M_['target'])\n",
    "    feature_inds = list(M_['feature_ind'])\n",
    "    features = F[feature_inds, :]\n",
    "    x = pd.DataFrame(features)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_sketches_permute(train_rep, test_rep):\n",
    "    summary_df = pd.DataFrame()\n",
    "    \n",
    "    # first train on the train_rep sketches\n",
    "    X_train_, y_train_= get_x_and_y_permute(M, train_rep)\n",
    "    logreg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "    #clf = CalibratedClassifierCV(log) \n",
    "    logreg.fit(X_train_, y_train_)\n",
    "#         svc = LinearSVC(random_state=0)\n",
    "#         clf = CalibratedClassifierCV(svc) \n",
    "#         clf.fit(X_train_, y_train_)\n",
    "\n",
    "    # for each repetition, predict from features, record accuracy and class probability    \n",
    "    for rep in test_rep:\n",
    "        X_test, y_test = get_x_and_y_permute(M, rep)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred) # temporarily just do accuracy \n",
    "#         prob = logreg.predict_proba(X_test)\n",
    "#         prob_df_svc = pd.DataFrame(prob)\n",
    "#         prob_df_svc.columns=list(logreg.classes_)\n",
    "#         prob_df_svc['target'] = y_test\n",
    "#         prob_list = []\n",
    "#         prob_df_svc = prob_df_svc.reset_index(drop=True)\n",
    "#         for j, d in prob_df_svc.iterrows():\n",
    "#             target = d['target']\n",
    "#             class_prob = d[target]\n",
    "#             prob_list.append(class_prob)\n",
    "        mean_prob = 1#np.mean(prob_list)\n",
    "\n",
    "        df_to_add = pd.DataFrame([[i, rep, acc, mean_prob]], columns=['iteration','test_rep', 'accuracy', 'class_prob'])\n",
    "        summary_df = summary_df.append(df_to_add)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_combined_df_permute = pd.DataFrame()\n",
    "random_seed = 4000000000\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print (str(i) + \"th iteration\")\n",
    "    pseudo_rep_list = []\n",
    "    for g in list(M['gameID'].unique()):\n",
    "        for t in list(M[M['gameID'] == g]['target'].unique()):\n",
    "            mini_df = M[(M['gameID'] == g) & (M['target'] == t)] \n",
    "            rep_list = np.array(mini_df['repetition'])\n",
    "            # pass in different random seed for each one \n",
    "            np.random.seed(random_seed)\n",
    "            shuffled_rep_list = np.random.permutation(rep_list)\n",
    "            pseudo_rep_list = pseudo_rep_list + list(shuffled_rep_list)\n",
    "            random_seed = random_seed - 1\n",
    "    M['pseudo_repetition'] = pseudo_rep_list\n",
    "    summary_df0 = classify_sketches_permute(0, [1,7])\n",
    "    summary_df7 = classify_sketches_permute(7, [6,0])\n",
    "    summary_df0['train_rep'] = [0] * summary_df0.shape[0]\n",
    "    summary_df7['train_rep'] = [7] * summary_df7.shape[0]\n",
    "    summary_df_list = [summary_df0, summary_df7]\n",
    "    combined_df_permute = pd.concat(summary_df_list, axis=0)\n",
    "    combined_df_permute = combined_df_permute.reset_index(drop=True)\n",
    "    distance_list = []\n",
    "    for j,d in combined_df_permute.iterrows():\n",
    "        distance_list.append(abs(d['test_rep']-d['train_rep']))\n",
    "    combined_df_permute['distance'] = distance_list\n",
    "    combined_df_permute = combined_df_permute.reset_index(drop=True)\n",
    "    final_combined_df_permute = final_combined_df_permute.append(combined_df_permute)#add(combined_df_permute, fill_value=0)\n",
    "result = final_combined_df_permute / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plt.figure(figsize=(5,5))\n",
    "g = sns.pointplot(\n",
    "    data=final_combined_df_permute,\n",
    "    x='distance',\n",
    "    y='accuracy',\n",
    "    hue='train_rep',\n",
    "    palette=\"Set1\")\n",
    "g.set(ylim=(0.5, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.pointplot(\n",
    "    data=final_combined_df_permute,\n",
    "    x='distance',\n",
    "    y='accuracy',\n",
    "    hue='train_rep',\n",
    "    palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make either training set or testing set (all sketches used for either one of them)\n",
    "from sklearn.metrics import log_loss\n",
    "def get_x_and_y(M, rep):\n",
    "    M_= M[M['repetition'] == rep]\n",
    "    y = list(M_['target'])\n",
    "    feature_inds = list(M_['feature_ind'])\n",
    "    features = F[feature_inds, :]\n",
    "    x = pd.DataFrame(features)\n",
    "    return x, y\n",
    "\n",
    "def get_training_set_error(train_rep):\n",
    "    X_train, y_train = get_x_and_y(M, train_rep)\n",
    "    targets = pd.Series(y_train).unique()\n",
    "    logreg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    #y_pred = logreg.predict(X_test)\n",
    "    y_pred = logreg.predict_proba(X_train)\n",
    "    # train classifier on these sample sketches from repetition (train) and test on sketches from game g repetition (test)\n",
    "    classification_loss = log_loss(y_train, y_pred, labels=targets)\n",
    "    return classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games = list(M['gameID'].unique())\n",
    "#targets = list(M['target'].unique())\n",
    "number_of_games = len(games)\n",
    "\n",
    "bootstrap_samples = []\n",
    "for i in range(1000):\n",
    "    sample = np.random.choice(number_of_games, number_of_games)\n",
    "    bootstrap_samples.append(sample)\n",
    "    \n",
    "def get_error(train_rep, test_rep):\n",
    "    \n",
    "    #grand_total_classification_loss = 0\n",
    "    err_df = pd.DataFrame()\n",
    "    for g in games:\n",
    "        print(\"computing average classification loss for game \" + g + \" with train rep: \" + str(train_rep) + \" and test rep: \" + str(test_rep))\n",
    "\n",
    "        samples_without_g = [sample for sample in bootstrap_samples if games.index(g) not in sample] \n",
    "        total_classification_loss = 0\n",
    "        training_set_error = get_training_set_error(train_rep)\n",
    "        sample_index=0\n",
    "        for sample in samples_without_g:\n",
    "            games_ = pd.Series(games)\n",
    "            sample_games = games_[sample] # sample is just a list of indices, so get actual games \n",
    "            sample_games = list(sample_games)\n",
    "            M_train = M[M['gameID'].isin(sample_games)]\n",
    "            X_train, y_train = get_x_and_y(M_train, train_rep)\n",
    "            targets = pd.Series(y_train).unique()\n",
    "            M_test = M[M['gameID'] == g]\n",
    "            X_test, y_test = get_x_and_y(M_test, test_rep)\n",
    "            logreg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "            logreg.fit(X_train, y_train)\n",
    "            #y_pred = logreg.predict(X_test)\n",
    "            y_pred = logreg.predict_proba(X_test)\n",
    "            # train classifier on these sample sketches from repetition (train) and test on sketches from game g repetition (test)\n",
    "            classification_loss = log_loss(y_test, y_pred, labels=targets)\n",
    "            total_classification_loss = total_classification_loss + classification_loss\n",
    "            sample_index=sample_index+1\n",
    "        average_classification_loss = total_classification_loss / len(samples_without_g)\n",
    "        error = 0.632 * average_classification_loss + 0.368 * training_set_error\n",
    "        df_to_add = pd.DataFrame([[error, g, sample_index, train_rep, test_rep]], columns=['error', 'gameID', 'sample_index', 'train_rep', 'test_rep'])\n",
    "        err_df = err_df.append(df_to_add)\n",
    "        #grand_total_classification_loss = grand_total_classification_loss + average_classification_loss\n",
    "    #grand_average_classification_loss = grand_total_classification_loss / len(games)\n",
    "\n",
    "    #error_632 = 0.632 * grand_average_classification_loss + 0.368 * training_set_error \n",
    "    return err_df\n",
    "\n",
    "# df_0_1 = get_error(0,1)\n",
    "# df_0_1.to_csv(\"df_0_1.csv\")\n",
    "df_0_7 = get_error(0,7)\n",
    "df_0_7.to_csv(\"df_0_7.csv\")\n",
    "df_7_6 = get_error(7,6)\n",
    "df_7_6.to_csv(\"df_7_6.csv\")\n",
    "df_7_0 = get_error(7,0)\n",
    "df_7_0.to_csv(\"df_7_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0_1 = pd.read_csv(\"df_0_1.csv\")\n",
    "all_err_df = pd.concat([df_0_1, df_0_7, df_7_6, df_7_0])\n",
    "all_err_df.to_csv(\"all_err_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to and add distance \n",
    "all_err_df = pd.read_csv(\"all_err_df.csv\")\n",
    "distance_list = []\n",
    "for j,d in all_err_df.iterrows():\n",
    "    distance_list.append(abs(d['test_rep']-d['train_rep']))\n",
    "all_err_df['distance'] = distance_list\n",
    "\n",
    "sns.set_context(\"paper\")\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.pointplot(\n",
    "    data=all_err_df,\n",
    "    x='distance',\n",
    "    y='error',\n",
    "    hue='train_rep',\n",
    "    palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
